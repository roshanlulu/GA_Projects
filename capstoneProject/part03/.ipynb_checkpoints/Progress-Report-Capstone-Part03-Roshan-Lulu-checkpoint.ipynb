{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Report - Capstone Part 03\n",
    "*Project Summary of Aviation Accident Analysis*\n",
    "### Submitted by Roshan Lulu\n",
    "*https://roshanlulu.github.io/*\n",
    "___\n",
    "![](./assets/QantasLead.jpg)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Week7.png\" alt=\"Drawing\" style=\"width:60px\" align=\"left\"/>\n",
    "\n",
    "# **<span>&nbsp;&nbsp;Throwback to Capstone part 02</span>**\n",
    "___\n",
    "> \n",
    "- Obtained primary dataset from `www.planecrashinfo.com`\n",
    "- Peformed Data cleaning and Exploratory data analysis.\n",
    "- During EDA, It was found that the dataset had quite a few missing values. As a result of which, a potential dataset needed to be taken into consideration for part 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Week9.png\" alt=\"Drawing\" style=\"width:60px\" align=\"left\"/>\n",
    "\n",
    "# **<span>&nbsp;&nbsp;Back to Week 9 - Capstone part 03</span>**\n",
    "___\n",
    "\n",
    "> ** I have divided this phase into 3 notebooks. Further ahead is the summary of all the notebooks **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Notebook 1 - eda_aviationnet_db`**\n",
    "___\n",
    "\n",
    "- Imported dataset from the Aviation Safety Network Website: `https://aviation-safety.net/database/`\n",
    "- Performed Initial Data cleaning/wrangling and EDA on the raw dataset.\n",
    "- Examined the data - There are 37 features\n",
    "- Check the null counts and remove nulls. This is a very important step for further EDA\n",
    "- Check the datatypes and convert them where required.\n",
    "- Clean each feature and perform Univariate data analysis: Use matplotlib/seaborn for plotting\n",
    "- Feature Engineering: Grouping data into categories were required\n",
    "- Feature Extraction: Extracting new features from the existing features\n",
    "- Evaluate features required for this project\n",
    "- Save the cleaned dataset to a csv file. This will be used as the dataset reference during the phase of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Summary of EDA\n",
    "*Categorizing the features during Data munging was helpful to decide the useful features for the project. The features I have retained, extracted or dropped have been described below. *\n",
    "\n",
    "> **Airline details: **\n",
    "- ['LeasedFrom', 'OperatedBy', 'OperatingFor', 'Operator', 'OnBehalfOf']\n",
    "- There were 5 columns that indicated the operator or the airline. Checking the null counts and description of these columns I decided to merge Operator and Operatingfor into 1 column and drop the other 3\n",
    "- Distinct Operators 2748. This is a lot! But, at this stage I do not want to drop the operators with least counts. Because the crash locations might still be important. \n",
    "- I was interested in the operator with highest accidents.\n",
    "\n",
    ">![operator](./gen_plots/operator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Accident Details**\n",
    ">\n",
    "- 'Nature' - The nature of the flight. The categories were quite dispersed and I was able to group them logically and create lesser groups\n",
    "\n",
    ">![FlightNature](./gen_plots/nature_of_flight.png)\n",
    "- 'Type' - I had a look at the famous aircraft manufacturers to ensure that I did not move it to the Others category. All the manufacturers with less than 10 crashes hvebeen grouped as the Others category\n",
    "The names had o be cleaned to remove the exact mdoel number as I am primarily interested in the manufacturer performance.\n",
    "\n",
    ">![AircraftType](./gen_plots/AircraftType.png)\n",
    "- 'FlightNumber' - It is kept as a unique ID\n",
    "- 'CarrierNumber' - Dropped it!\n",
    "> - 'FirstFlight' - extracted the year that the particular aircraft mentioned was flown the first time.\n",
    "    I have kept this as it would be a good indicator to calculate age of the airline at the time of the accident.\n",
    "> ![FirstFlight](./gen_plots/FirstFlight.png)\n",
    "- 'Registration' - Dropped it!  \n",
    "- 'Number' - Dropped it!\n",
    "- 'Engines' - Split this feature into No of engines and Engine Count\n",
    "![EngineType](./gen_plots/EngineType.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fatalities**\n",
    "- ['Crew','Passengers','CollisionCasualties','TotalFatalities','GroundCasualities'] \n",
    "- These features were converted to integer datatype and renamed. It is a good feature for visualizing and modelling at a later stage.\n",
    ">![FirstFlight](./gen_plots/PassengerFatal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Crash Details**\n",
    "- 'CrashSiteElevation' - This provides the elevation at which crashes occured. I have cleaned the feature and retained it.\n",
    "- 'Phase' - The phase during which the accident occured is mentioned here. It is quite an interesting aspect.\n",
    "![AccidentByPhase](./gen_plots/AccidentByPhase.png)\n",
    "- 'TotalAirFrameHrs','Duration' - Drop it\n",
    "- 'Date' - Split into Day, Date, Month, year\n",
    "![AircraftDamage](./gen_plots/YearCountCloud.png)\n",
    "- 'Narrative' - Kept to be analysed later\n",
    "- 'DepartureAirport' - I have not decided what to do with this column. But Ill keep it\n",
    "- 'DestinationAirport'- I have not decided what to do with this column. But Ill keep it\n",
    "- 'AirplaneDamage' - Describes the damage to the airplane after the incident.\n",
    "- 'AirplaneFate' - Described the fate of the airplane after the accident\n",
    "![AircraftDamage](./gen_plots/AircraftDamageCount.png)\n",
    "- 'Time' - Time of the accident\n",
    "- 'Cycles' - Drop it\n",
    "- 'Location' - Accident Location. Extracted it into Sublocation(Exact Location) and Location(Country names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Investigation Results** \n",
    "- I am not sure if this category is useful. But I'll keep the features with more valid values values. It can be dropped later if it is not used.\n",
    "- 'InvestigatingAgency' - Keep it\n",
    "- 'Status' - Kept it\n",
    "- 'Released' - Drop it\n",
    "- 'DownloadReport' - Drop it\n",
    "- 'Issued' - Drop it\n",
    "- 'DurationOfInvestigation' - Keep it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Latitude Longitude Extraction**\n",
    "- Now that we have the location in terms of country, an important feature to extract is the Latitude Longitude information.\n",
    "- The Google Geocode Api was used to extract Latitude Longitudes from the Locations. This is also a very important feature for the analysis.\n",
    "- This portion will only be run once to get the latlong values. After that only the saved file will be used. APIs are not meant to be called everytime a notebook is run.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **` Notebook 2 - data_map_viz`**\n",
    "> \n",
    "- In the previous notebook I was able to use matplotlib and seaborn for the univariate and bivariate analysis.\n",
    "- Further, Since the data is distributed across the world, I am interested in getting a world view of the incidents.\n",
    "- I was able to use two plottong techniques: **Plot.ly and Folium**. These are open source map plot libraries. I would definitely be exploring more of this for the next phase. \n",
    "\n",
    "> ![mapit_country](./gen_plots/map_plot_heat.png \"Title\")\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Notebook 3 - text_analytics_modelling`**\n",
    "- The accident narrative in the dataset was quite informative. In order to be able to use it efficiently, I decided to use Natural Language Processing on it.\n",
    "- Along with analysing the patterns in accidents based on different features, I thought it would be interesting to classify the incidents based on the narrative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> \n",
    "**Text Pre-Processing - Noise Removal**\n",
    ">\n",
    "- Text is one of the most unstructured forms of data and Noise removal is a very important step before any data can be analysed. So is the case for text.\n",
    "> \n",
    "- Noise is information that either corrupts your data or is not useful. Below are some of the common forms of noise in text and that I will be employing going ahead:\n",
    "    1. Stop words are the most commonly used words in a language. i.e the words that do not convey meaniful information if on its own. eg: The, A, An, Is, for etc\n",
    "    2. Punctuations also do not convey any information from an analytics point of view. Hence, it can also be considered as Noise.\n",
    ">\n",
    "*Noise(text) = Stopwords(text language) + Punctuation(text)*\n",
    "\n",
    "> \n",
    "**Text Normalisation**\n",
    "> \n",
    "- Normalisation of data is converting different forms of the same word into a common one. Two of the commonly used forms are :\n",
    "    - 1. Stemming i.e. stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "        - I tried using Stemming for my text, but it stripped the word of all its meaning, hence currently I am not using stemming. \n",
    "    - 2. Lemmatization i.e. an organised and step by step procedure of obtaining the root form of the word, it makes use of vocabulary and word structure and grammar relations.\n",
    "        - Lemmatization, did not help me much. For now, I am not using these functions. But at a later point I will come back to it if required.\n",
    "> \n",
    "**Feature Extraction**\n",
    "- Count Vectorizer is used to get the most commonly used vocabulary. Count vectorizer is one of the basic techniques when dealing with textual content. It is the process of getting a word count of the text/document. The parameters of a count vectorizer can be modified depending on the problem at hand.\n",
    "\n",
    "> ![frequentwords](./gen_plots/wordcount.png)\n",
    "> \n",
    "**Unsupervised Topic Modelling using LDA (Latent Dirichlet Allocation)**\n",
    "> \n",
    "- I am interested to divide the topics based on the cause of accidents. Hence Clustering them based on the description is the next task. There are many approaches for obtaining topics from a text. \n",
    "    - Latent Dirichlet Allocation is the most popular topic modeling technique and I am planning to try that first.\n",
    "> \n",
    "**Cleaning and Preprocessing**\n",
    "> \n",
    "- Cleaning is an important step before any text mining task. Since this has already been done above. we wont be looking at it.\n",
    "> \n",
    "**Preparing Document-Term(DT) Matrix**\n",
    "> \n",
    "- All the text documents combined is known as the **corpus**. To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the DT matrix.\n",
    "> \n",
    "**Running LDA Model**\n",
    "> \n",
    "- Next step is to create an object for LDA model. The dataset does not have enough information to divide it into train and test sets. To begin, I will be using the LDA model to come up with similar words to create topics for the incidents. This can also be seen as feature engineering using the model. I expect to see some interesting results with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Summary**\n",
    "> \n",
    "The topic modelling results are not very convincing. It requires more iterations before it can come up with meaningful distributions. Few of the options I am planning to try are:\n",
    "> \n",
    "- I think the stop words list needs to be expanded such that relevant words are picked by LDA model.\n",
    "- I have seen that Topic extraction with Non-negative Matrix Factorization is an alternate method for LDA. I would like to try that and see its results too.\n",
    "- SVM is supposedly a good way to check the accuracy of the LDA results. But the challenge is that the results cannot be compared to any training set. \n",
    "- If the topic results from modelling seem convincing enough, I will go ahead and use them. The idea is to add the extracted topics as a feature into the dataset. With this as a new feature, it would be an extra addition to analyzing the aviation accidents.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Lessons Learnt ..\n",
    "<img src=\"./assets/road_ahead2.jpg\" alt=\"Drawing\" style=\"width:200px\" align=\"middle\"/> \n",
    "\n",
    "**Success**\n",
    "> \n",
    "- I am quite happy with the analysis of the dataset till now. It is quite a raw dataset hence requires more Feature extraction, Feature engineering to come up with a clean feature. EDA during this phase struck the realization that I am quite comfortable using pandas dataframes and plotting using seaborn and matplotlib. Ofcourse there are tons of things out there, but it is a good start. \n",
    "- During this phase, I got my hands messy with the following topics. These are things I have not explored during any of the projects.\n",
    "    - Geopy\n",
    "    - Plot.ly\n",
    "    - Folium\n",
    "    - GoogleV3 for google api usage\n",
    "    - Text Modelling using LDA\n",
    "    - Basics of Natural Language Processing\n",
    "\n",
    "**Setbacks**\n",
    "> \n",
    "- I have tried methods for Text Modelling on the Narrative. The results are not very satisfactory. So i want to try out the new models and techniques for handling text that were taught during NLP lessons in Week 9.\n",
    "- Cleaning was not as easy as expected. When I think I'm done cleaning it comes back again. I learnt a very important lesson that more time is required for cleaning. It is difficult to build on top of a dirty basee. I think this thought will get me a long way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <span>The Road ahead... </span>\n",
    "<img src=\"./assets/road_ahead1.jpeg\" alt=\"Drawing\" style=\"width:200px\" align=\"middle\"/>\n",
    "**Plan 1**: \n",
    "> \n",
    "- The airline accident database does not have any specific predictor. These kinds of datasets generally use Unsupervised machine learning algorithms i.e to get a deeper understanding about the subject. \n",
    "- K means, Heirarchichal clustering are some of the methods that that I would like to try using.\n",
    "> \n",
    "**Challenge**: The evaluation metrics fo runsupervised learning are not clearly defined. Silhouette score is a possible metric, but again that is generally done considering the truth label is already present.\n",
    "\n",
    "\n",
    "**Plan 2**:\n",
    "> \n",
    "- Further Feature engineering\n",
    "    - Identify Hemisphere(Based on country)\n",
    "    - Identify seasons based on hemisphere and Month of accident\n",
    "> \n",
    "- **ToDo**: Check correlation between hemisphere, month, no of accidents, fatalities, operator, aircraft.\n",
    "    - Add some visual plots for the relations\n",
    "> \n",
    "- **Challenge**: Is there any mdoel that can be applied? What are the other metrics that can be checked?\n",
    "    \n",
    "**Plan 3**\n",
    "> \n",
    "- **Improve Text modelling**:\n",
    "    - I need to check how the NLP techniques learnt in Week 9 can help with this.\n",
    "    - Certain libraries like spaCy could be used for better topic mdoelling. \n",
    "    - I would like to use TFIDF vectorizer to get meaningful topics.\n",
    "    - SVM is supposedly a good way to check the accuracy of the LDA results. But the challenge is that the results cannot be compared to any training set.\n",
    "    - Since the data does not have truth label, I am thinkig if It is a good idea to create a small training set for the topics. The fit a model for the remaining dataset. Thsi would be based on assumption and if it is not against the rules, then it would be worth a try.\n",
    "    - Topics in mind to divide the accidents: Hijack activity, Weather conditions, Technical Error, Pilot error, Military activity\n",
    "    \n",
    "**Plan 4**\n",
    "> \n",
    "- **ToDo**: The Location of accident has been divided into Country and the Sublocation. In this phase, I have managed to extract the Lat-Lon's of the Counries and plot them. I do want to plot the Lat-Lon's of the exact location. This would give a better perspective.\n",
    "- **Challenge**: Can I use clustering using the Lat-Lon's? Hm, not sure on how to go ahead with it.\n",
    "\n",
    "**Plan 5**\n",
    "> \n",
    "- The Departure and Destination airport codes have been extracted. Still thinking if I can use it to extract some good information. \n",
    "- One option could be to calculate the flight distance after getting the Lat-Lon's. Need ot figur eout what I can do with it.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
